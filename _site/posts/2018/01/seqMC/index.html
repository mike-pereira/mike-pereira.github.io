

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Sequential Monte Carlo: Introduction and advances - Mike Pereira</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Mike Pereira">
<meta property="og:title" content="Sequential Monte Carlo: Introduction and advances">


  <link rel="canonical" href="http://localhost:4000/posts/2018/01/seqMC/">
  <meta property="og:url" content="http://localhost:4000/posts/2018/01/seqMC/">



  <meta property="og:description" content="This note is an attempt to convey the main ideas and notions of the talk entitled “Sequential Monte Carlo: Introduction and advances” given by Nicolas Chopin (ENSAE, Paris, France) during the Stochastic algorithm Day 2017 (Journée algorithmes stochastiques 2017). This conference was held at Université Paris Dauphine (Paris, France) on December 1st, 2017.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-01-18T00:00:00-08:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Mike Pereira",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Mike Pereira Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Mike Pereira</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/mike.png" class="author__avatar" alt="Mike Pereira">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Mike Pereira</h3>
    <p class="author__bio">Assistant professor (Enseignant-chercheur, Tenure track), Mines Paris - PSL</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Fontainebleau, France</li>
      
      
      
      
        <li><a href="mailto:mike.pereira@minesparis.psl.eu"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
        <li><a href="https://www.researchgate.net/profile/Mike-Pereira"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i> ResearchGate</a></li>
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/mike-pereira"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=96uBOBMAAAAJ&hl=en&oi=ao"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-7899-2690"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Sequential Monte Carlo: Introduction and advances">
    <meta itemprop="description" content="This note is an attempt to convey the main ideas and notions of the talk entitled “Sequential Monte Carlo: Introduction and advances” given by Nicolas Chopin (ENSAE, Paris, France) during the Stochastic algorithm Day 2017 (Journée algorithmes stochastiques 2017). This conference was held at Université Paris Dauphine (Paris, France) on December 1st, 2017.">
    <meta itemprop="datePublished" content="January 18, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Sequential Monte Carlo: Introduction and advances
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  5 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-01-18T00:00:00-08:00">January 18, 2018</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>This note is an attempt to convey the main ideas and notions of the talk entitled “Sequential Monte Carlo: Introduction and advances” given by Nicolas Chopin (ENSAE, Paris, France) during the Stochastic algorithm Day 2017 (<em>Journée algorithmes stochastiques 2017</em>). This conference was held at Université Paris Dauphine (Paris, France) on December 1st, 2017.</p>

<h2 id="feynmac-kac-models">Feynmac-Kac models</h2>

<p>A Feynman-Kac model is made of :</p>

<ul>
  <li>a Markov chain in \(\mathcal{X}\) with
    <ul>
      <li>Initial law : \(dx_0\in\mathcal{X}\mapsto m_0(dx_0)\)</li>
      <li>Markov kernel (at iteration \(t\)) : \(dx_t \in\mathcal{X} \mapsto m_t(x_{t-1},dx_t)=\pi(x_t\vert x_{t-1})\)</li>
    </ul>
  </li>
  <li>a sequence of potential functions \(G_0 : \mathcal{X} \rightarrow \mathbb{R}^+\), \(G_t : \mathcal{X}\times \mathcal{X} \rightarrow \mathbb{R}^+\)</li>
</ul>

<p>The aim is to compute sequentially quantities of the form :</p>

\[\begin{aligned}\mathbb{Q}_t(\phi)=\frac{1}{Z_t}\mathbb{E}\left[ \phi(X_t)G_0(X_0)\prod\limits_{s=1}^tG_s(X_{s-1},X_s)\right]\end{aligned}\]

<p>where</p>

\[\begin{aligned}
Z_t=\mathbb{E}\left[G_0(X_0)\prod\limits_{s=1}^tG_s(X_{s-1},X_s)\right].
\end{aligned}\]

<p><strong>Example:</strong>
If \(G_t(x_{t-1},x_t):=\boldsymbol{1}_{A_t}(x_t)\), then
\(Z_t = \mathbb{P}(\forall s\le t, \; X_s\in A_s)\) and
\(\mathbb{Q}_t = \pi(X_t \vert X_s\in A_s : s \le t)\) Notice in
particular that</p>

\[\begin{aligned}
Z_t=\int \underbrace{\left(\int G_0(x_0)\prod\limits_{s=1}^tG_s(x_{s-1},x_s)\pi(x_0,...,x_t)dx_0...dx_{t-1}\right)}_{:=p_t(x_t)}dx_t
\end{aligned}\]

\[\begin{aligned}
\mathbb{Q}_t(\phi)&amp;= \frac{1}{Z_t}\int \phi(x_t)\left(\int G_0(x_0)\prod\limits_{s=1}^tG_s(x_{s-1},x_s)\pi(x_0,...,x_t)dx_0...dx_{t-1}\right)dx_t\\
&amp;= \frac{1}{Z_t}\int \phi(x_t)p_t(x_t)dx_t=\int \phi(x_t)\underbrace{\frac{p_t(x_t)}{\int p_t(x_t)dx_t}}_{:=\mathbb{Q}_t(x_t)}dx_t=\int \phi(x_t)\mathbb{Q}_t(x_t)dx_t
\end{aligned}\]

<p>Then \(\mathbb{Q}_t(x_t)\) can be seen as a probability density and
\(\mathbb{Q}_t(\phi)\) as the expectation of \(\phi(X_t)\) when
\(X_t \sim \mathbb{Q}_t(x_t)\).</p>

<p><strong>Example:</strong>
Hidden Markov models</p>

<p>Imagine that the Markov chain \((X_t)\) is observed through</p>

\[\begin{aligned}
Y_t=h(X_t)+\text{ noise }
\end{aligned}\]

<p>and take \(G_t(x_{t-1},x_t)=\pi(y_t\vert x_t)\) the density of \(Y_t\) conditional on \(X_t=x_t\). Then,</p>

\[\begin{aligned}
p_t(x_t)&amp;=\int \prod_{s=0}^t \pi(y_s\vert x_s\pi(x_0,...,x_t)dx_0...dx_{t-1}\\
&amp;=\int\pi(y_0,...,y_t,x_0,...,x_t)dx_0...dx_{t-1}=\pi(y_0,...,y_t,x_t)
\end{aligned}\]

<p>And therefore</p>

\[\begin{aligned}
Z_t=\int \pi(y_0,...,y_t,x_t)dx_t=\pi(y_0,...,y_t)
\end{aligned}\]

<p>is the likelihood of the observations and</p>

\[\begin{aligned}
\mathbb{Q}_t(x_t)=\pi(y_0,...,y_t,x_t)/\pi(y_0,...,y_t)=\pi(x_t\vert y_0,...,y_t)
\end{aligned}\]

<p>is the law of \(X_t\) conditional on the observations.</p>

<h2 id="particle-filtering">Particle Filtering</h2>

<p>Particle filtering is used to approximate the probability measure
\(\mathbb{Q}_t\) sequentially</p>

<ul>
  <li>
    <p>Initialization : \(t=0\)</p>

    <ul>
      <li>
        <p>Generate \(N\) samples from \(m_0(dx_0)\) :
\(X_0^{(i)} \sim m_0(dx_0) : i\in[\![1,N]\!]\)</p>
      </li>
      <li>
        <p>Compute initial weights :
\(W_0^{(i)} =G_0(X_0^{(i)} )/\sum\limits_{j=1}^N G_0(X_0^{(j)} ) : i\in[\![1,N]\!]\)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>For time \(t&gt;0\) :</p>

    <ul>
      <li>
        <p>Do for \(i\in [\![1,N]\!]\) :</p>

        <ul>
          <li>
            <p>Draw an index \(A_{t-1}^{i}\) from \([\![1,N]\!]\) with
probabilities
\(\mathbb{P}\left(A_{t-1}^{i}=k\right)\propto W_{t-1}^{(k)}\)</p>
          </li>
          <li>
            <p>Generate
\(X_t^{(i)} \sim m_t\left(X_{t-1}^{\left(A_{t-1}^{i}\right)}, dx_t\right)\)</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Compute the new weights :
\(W_t^{(i)} =G_t(X_{t-1}^{\left(A_{t-1}^{i}\right)},X_t^{(i)} )/\sum\limits_{j=1}^N G_t(X_{t-1}^{\left(A_{t-1}^{j}\right)},X_t^{(j)} )\)</p>
      </li>
    </ul>
  </li>
</ul>

<p>The quantities of interest are computed using</p>

\[\begin{aligned}
\mathbb{Q}_t^N(\phi)=\sum\limits_{i=1}^N W_t^{(i)}\phi(X_t^{(i)}) \approx \mathbb{Q}_t(\phi)
\end{aligned}\]

<p>and</p>

\[\begin{aligned}
Z_t^N=\prod\limits_{s=0}^t\left[\frac{1}{N}\sum\limits_{i=1}^N G_t(X_{t-1}^{\left(A_{t-1}^{i}\right)},X_t^{(i)} ) \right]\approx Z_t
\end{aligned}\]

<figure>
  <img src="/images/seqMC/part_schema.png" />
  <figcaption>Particle filtering in a sketch.</figcaption>
</figure>

<p>In this particle filter formulation, the particles are generated using
the kernel \(m_t(x_{t-1},dx_t)\). In practice, this generating kernel
doesn’t have to match the kernel \(f_t(x_{t-1},dx_t)\) of the Markov chain
of interest, but when it is the case, the corresponding PF is called a
bootstrap filter.</p>

<p>In particular, in the Hidden Markov models, if \(f_t\) is the Markov
kernel of the underlying process \((X_t)\) then, for any kernel \(m_t\), if
the potential functions are of the form</p>

\[\begin{aligned}
G_0(x_0)=\frac{f_0(x_0)\pi(y_0\vert x_0)}{m_0(x_0)} \;,  \quad G_t(x_{t-1},x_t)=\frac{f_t(x_{t-1},dx_t)\pi(y_t\vert x_t)}{m_t(x_{t-1},dx_t)}
\end{aligned}\]

<p>then \(\mathbb{Q}_t\) will still match the filtering
distribution, i.e. the law of \(X_t\) conditional to the data.</p>

<p><em>Proof.</em> For a Markov chain \((X_t)\) with kernel \(f_t\) :</p>

\[\begin{aligned}
\pi(x_0,...,x_t)=\pi(x_0)\prod\limits_{s=1}^t\pi(x_s\vert x_{s-1})=f_0(x_0)\prod\limits_{s=1}^t f_s(x_{s-1},dx_s)
\end{aligned}\]

<p>Then, the Feynman-Kac model with Markov chain \((\tilde X_t)\) with kernel
\(m_t\) and potential functions \(G_t\) defined above
verifies</p>

\[\begin{aligned}
\tilde p_t(\tilde X_t=\tilde x_t)&amp;=\int \frac{f_0(\tilde x_0)\pi(y_0 \vert \tilde x_0)}{m_0(\tilde x_0)}\prod_{s=1}^t \frac{f_s(\tilde x_{s-1},d\tilde x_s)\pi(y_s\vert \tilde x_s)}{m_s(\tilde x_{s-1},d\tilde x_s)}\pi(\tilde x_0,...,\tilde x_t)d\tilde x_0...d\tilde x_{t-1}\\
&amp;= \int f_0(\tilde x_0)\pi(y_0 \vert \tilde x_0)\prod_{s=1}^t f_s(\tilde x_{s-1},d\tilde x_s)\pi(y_s\vert \tilde x_s)d\tilde x_0...d\tilde x_{t-1}\\
&amp;= \int \pi(x_0,...,x_t)\prod_{s=0}^t \pi(y_s\vert x_s)dx_0...d x_{t-1} \\
&amp;= \int \pi(x_0,...,x_t) \pi(y_0,...,y_t\vert x_0,..., x_t)dx_0...d x_{t-1}\\
&amp;=\int\pi(y_0,...,y_t,x_0,...,x_t)dx_0...dx_{t-1} \\
&amp;=\pi(y_0,...,y_t,x_t)=p_t(X_t=x_t)
\end{aligned}\]

<p>Therefore the probability \(\tilde{\mathbb{Q}}_t\) associated with the
chain \((\tilde X_t)\) is exactly the filtering distribution of the
underlying process \((X_t)\). ◻</p>

<p>The generating kernel \(m_t\) is chosen to minimize the variance of \(G_t\): the optimal choice is then the distribution of \(X_t\) conditional to
\(X_{t-1}\) and \(Y_t\) (intractable in general…).</p>

<h2 id="resampling">Resampling</h2>

<p>Resampling is the process that takes as an input a weighted sample
\(\lbrace (X^{(i)},W^{(i)})\rbrace_{i\in[\![1,N]\!]}\) and returns
resampled variables \(\lbrace X^{(A^i)} \rbrace_{i\in[\![1,N]\!]}\) where
\(A^i\) is a random index in \([\![1,N]\!]\). A good resampling scheme
guarantees</p>

\[\begin{aligned}
\frac{1}{N}\sum\limits_{i=1}^N\delta(X^{(A^i)})\approx \sum\limits_{i=1}^N W^n\delta(X^{(i)})
\label{GS}
\end{aligned}\]

<p>i.e. the empirical probability measure of the resampled
variables is close to the weighted empirical measure of the input
variables.</p>

<p>Denote
\(F_N : x\in\mathbb{R}_+ \mapsto \sum\limits_{i=1}^N W^{(i)}\boldsymbol{1}(x\ge i)\in[0,1]\)
the cumulative distribution function associated with the normalized
weights \((W^{(i)})\). Three common resampling schemes are described
below.</p>

<h3 id="multinomial-resampling">Multinomial resampling</h3>

<ul>
  <li>
    <p>For \(i\in [\![1,N]\!]\) :</p>

    <ul>
      <li>
        <p>Draw \(U^{(i)}\sim \mathcal{U}(0,1)\)</p>
      </li>
      <li>
        <p>Return \(A^i=F^{-1}_N(U^{i})\)</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="stratified-resampling">Stratified resampling</h3>

<ul>
  <li>
    <p>For \(i\in [\![1,N]\!]\) :</p>

    <ul>
      <li>
        <p>Draw \(U^{(i)}\sim \mathcal{U}(0,1)\)</p>
      </li>
      <li>
        <p>Return \(A^i=F^{-1}_N(\frac{i-1+U^{i}}{N} )\)</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="systematic-resampling">Systematic resampling</h3>

<ul>
  <li>
    <p>Draw \(U\sim \mathcal{U}(0,1)\)</p>
  </li>
  <li>
    <p>For \(i\in [\![1,N]\!]\) :</p>

    <ul>
      <li>Return \(A^i=F^{-1}_N(\frac{i-1+U}{N} )\)</li>
    </ul>
  </li>
</ul>

<figure>
  <img src="/images/seqMC/inv_cdf.png" />
  <figcaption>Example of inverse cumulative distribution $F_N$, $N=4$.
$\lbrace U^{(k)}, 1\le k\le 3\rbrace$ are samples drawn using
multinomial resampling.</figcaption>
</figure>

<p>Stratified and systematic resampling are used in practice because they
are a bit faster and lead to lower variance estimates numerically. But
for theory, multinomial resampling is preferred as it is easier to study
and constitutes a good sampler in the sense described at the beginning of this section. Hence, little
theoretical background is provided for stratified and systematic
sampling, regarding for instance their consistency.</p>

<h2 id="consistency-results">Consistency results</h2>

<p>A resampling scheme is consistent if it preserves weak convergence. A
theorem states that an unbiased resampling scheme is consistent if the
collection of variables</p>

\[\begin{aligned}
\left\lbrace \#^i:=\sum\limits_{j=1}^N\boldsymbol{1}(A^j=i)=\text{Card}\lbrace j\in[\![1,N]\!] : A^j=i\rbrace \right\rbrace_{i\in[\![1,N]\!]}
\end{aligned}\]

<p>is negatively associated, i.e., for every pair of disjoint subsets
\(I_1,I_2 \subset [\![1,N]\!]\),</p>

\[\begin{aligned}\text{Cov}\left(\varphi_1(\lbrace \#^i : i\in I_1\rbrace), \varphi_2(\lbrace \#^i : i\in I_2\rbrace)\right)\le 0
\end{aligned}\]

<p>for all non-decreasing functions
\(\varphi_k : \mathbb{R}^{|I_k|}\rightarrow \mathbb{R}\),
\(k\in\lbrace 1,2\rbrace\), such that the covariance is well-defined.<br />
This theorem is used to demonstrate that multinomial and residual
resampling are consistent (which was a known fact) but also stratified
and SSP resampling (which is a new fact). However, the theorem can’t be
applied to systematic resampling.</p>

<h2 id="references">References</h2>

<ul>
  <li>Nicolas Chopin (2017, December). <em>Sequential Monte Carlo: introduction,
recent advances</em>. Paper presented at "Journée algorithmes
stochastiques", Université Paris Dauphine, Paris, France</li>
</ul>


        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#feynmac-kac" class="page__taxonomy-item" rel="tag">Feynmac-Kac</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#particle-filtering" class="page__taxonomy-item" rel="tag">Particle filtering</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#sequential-monte-carlo" class="page__taxonomy-item" rel="tag">Sequential Monte Carlo</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/posts/2018/01/seqMC/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/2018/01/seqMC/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/posts/2018/01/seqMC/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="http://localhost:4000/posts/2018/01/mcmc_tall/" class="pagination--pager" title="Markov chain Monte Carlo and tall data
">Previous</a>
    
    
      <a href="http://localhost:4000/posts/2018/01/stoML/" class="pagination--pager" title="Stochastic algorithms in Machine Learning
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2019/10/pg2019/" rel="permalink">A few take-away messages from the conference Petroleum Geostatistics 2019
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  2 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-10-01T00:00:00-07:00">October 01, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>On September 2-6, 2019, I attended on behalf of <a href="https://estimages.com/">Estimages</a> (the company funding my PhD) the conference <a href="https://www.earthdoc.org/content/proceedings/florence2019">Petroleum Geostatistics 2019</a> in Florence, Italy. Here is my take on the messages that were delivered during this exciting conference.</p>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2018/01/stoML/" rel="permalink">Stochastic algorithms in Machine Learning
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  11 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-01-18T00:00:00-08:00">January 18, 2018</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>This note is an attempt to convey the main ideas and notions of the talk entitled “Stochastic algorithms in Machine Learning” given by Aymeric Dieuleveut (EPFL, Lausanne, Switzerland) during the Stochastic algorithm Day 2017 (<em>Journée algorithmes stochastiques 2017</em>). This conference was held at Université Paris Dauphine (Paris, France) on December 1st, 2017.</p>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/posts/2018/01/mcmc_tall/" rel="permalink">Markov chain Monte Carlo and tall data
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-01-18T00:00:00-08:00">January 18, 2018</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>This note is an attempt to convey the main ideas and notions of the talk entitled “On Markov chain Monte Carlo and tall data” given by Rémi Bardenet (CNRS, Lille, France) during the Stochastic algorithm Day 2017 (<em>Journée algorithmes stochastiques 2017</em>). This conference was held at Université Paris Dauphine (Paris, France) on December 1st, 2017.</p>

</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/mike-pereira"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Mike Pereira. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

